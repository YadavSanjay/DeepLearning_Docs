{\rtf1\ansi\ansicpg1252\cocoartf2512
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Bold;}
{\colortbl;\red255\green255\blue255;\red24\green24\blue24;\red255\green255\blue255;\red53\green118\blue190;
}
{\*\expandedcolortbl;;\cssrgb\c12157\c12157\c12157;\cssrgb\c100000\c100000\c100000;\cssrgb\c25882\c54510\c79216;
}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\margl1440\margr1440\vieww25480\viewh12740\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
NLP Research\
\pard\pardeftab720\partightenfactor0

\fs28 \cf2 \expnd0\expndtw0\kerning0
\
\cb3 NLP is a huge and rapidly emerging area. So to have an up-to-date understanding of its advances one should always keep track of what is going. In these reading material we provide some links for you that give a nice 
\f1\b overview of NLP trends as for the end of 2017
\f0\b0 .\cb1 \
\cb3 First, it is always a good idea to check out highlights from main conferences. There are nicely summarized trends of ACL-2017: {\field{\*\fldinst{HYPERLINK "http://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-1.html"}}{\fldrslt \cf4 \ul \ulc4 part 1}}, {\field{\*\fldinst{HYPERLINK "http://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-2.html"}}{\fldrslt \cf4 \ul \ulc4 part 2}}. Also, some highlights from EMNLP-2017 are available {\field{\*\fldinst{HYPERLINK "http://blog.aylien.com/highlights-emnlp-2017-exciting-datasets-return-clusters/"}}{\fldrslt \cf4 \ul \ulc4 here}}. Second, it would be a good idea to monitor some blogs, e.g. Sebastian Ruder has nice posts about {\field{\*\fldinst{HYPERLINK "http://ruder.io/deep-learning-nlp-best-practices/index.html"}}{\fldrslt \cf4 \ul \ulc4 DL in NLP}}, {\field{\*\fldinst{HYPERLINK "http://ruder.io/deep-learning-optimization-2017/index.html"}}{\fldrslt \cf4 \ul \ulc4 optimization trends}}, {\field{\*\fldinst{HYPERLINK "http://ruder.io/word-embeddings-2017/index.html#embeddingsformultiplelanguages"}}{\fldrslt \cf4 \ul \ulc4 word embeddings}}, and many others.\cb1 \
\cb3 One of still active topics is Thought Vectors and how one can interpret directions in the hidden space. E.g. you might be interested to check out {\field{\*\fldinst{HYPERLINK "http://gabgoh.github.io/ThoughtVectors/"}}{\fldrslt \cf4 \ul \ulc4 this post}}. However, it's getting more clear that compressing all the input into one vector is often not enough and one might make nice things with {\field{\*\fldinst{HYPERLINK "https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/"}}{\fldrslt \cf4 \ul \ulc4 attention and linguistic information}}. Some more tips about attention {\field{\*\fldinst{HYPERLINK "https://awni.github.io/train-sequence-models/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=The%20Wild%20Week%20in%20AI"}}{\fldrslt \cf4 \ul \ulc4 here}}.\cb1 \
\cb3 Finally, {\field{\*\fldinst{HYPERLINK "https://tryolabs.com/blog/2017/12/12/deep-learning-for-nlp-advancements-and-trends-in-2017/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=The%20Wild%20Week%20in%20AI"}}{\fldrslt \cf4 \ul \ulc4 this is}} another nice overview of 2017 trends in NLP research - advances in unsupervised machine translation seem especially exciting!\cb1 \

\f1\b \cb3 Not surprisingly, you will notice that each new year introduces new SOTA models and NLP techniques. Just to mention a few:
\f0\b0 \cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Transformers are everywhere, {\field{\*\fldinst{HYPERLINK "http://jalammar.github.io/illustrated-transformer/"}}{\fldrslt \cf4 \ul \ulc4 this blogpost }}might be a good place to start\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
New representation learning techniques (BERT, ELMO, etc).: {\field{\*\fldinst{HYPERLINK "http://jalammar.github.io/illustrated-bert/"}}{\fldrslt \cf4 \ul \ulc4 blogpost}}\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Big pre-trained LM models (Transformer-XL, GPT-2, etc): {\field{\*\fldinst{HYPERLINK "https://jalammar.github.io/illustrated-gpt2/?utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter"}}{\fldrslt \cf4 \ul \ulc4 blogpost}}\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 To conclude, we would like to say 
\f1\b thank you
\f0\b0  for taking our course and wish best of luck in your future NLP projects!}